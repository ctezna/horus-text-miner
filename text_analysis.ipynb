{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'the', '``', \"''\", \"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/carlostezna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')\n",
    "remove_terms = ['the', '``', \"''\", \"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would']\n",
    "stopwords = stopwords + remove_terms + list(string.punctuation)\n",
    "print(stopwords)\n",
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(document):\n",
    "        \"\"\"\n",
    "        Tokenizes documents then normalizes and lemmatizes tokens\n",
    "        \"\"\"\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        words = word_tokenize(document)\n",
    "        words_clean = []\n",
    "        for word in words: # Go through every word in your tokens list\n",
    "            w = word.lower()\n",
    "            if (w not in stopwords):  # remove stopwords and punctuation\n",
    "                words_clean.append(lemmatizer.lemmatize(w))\n",
    "        return words_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    f = open(file)\n",
    "    try:\n",
    "        raw = f.read()\n",
    "        return file.split('/')[-1], raw\n",
    "    except:\n",
    "        print(file)\n",
    "        pass\n",
    "\n",
    "def load_collection(files):\n",
    "    texts = []\n",
    "    for file in files:\n",
    "        doc_id, text = load_document(file)\n",
    "        texts.append(text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "COLLECTION_DIR = './dataset/newDataset/'\n",
    "files = [COLLECTION_DIR + file for file in os.listdir(COLLECTION_DIR)]\n",
    "\n",
    "corpus = load_collection(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# using default tokenizer in TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(token_pattern='(\\S+)', min_df=3, max_df=0.8, \n",
    "                        ngram_range=(1, 2), stop_words=stopwords, tokenizer=word_tokenize)\n",
    "features = tfidf.fit_transform(corpus)\n",
    "df = pd.DataFrame(\n",
    "        features.todense(),\n",
    "        columns=tfidf.get_feature_names()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'best</th>\n",
       "      <th>'bot</th>\n",
       "      <th>'bot nets</th>\n",
       "      <th>'cold</th>\n",
       "      <th>'cold calls</th>\n",
       "      <th>'could</th>\n",
       "      <th>'do</th>\n",
       "      <th>'eu</th>\n",
       "      <th>'goodbye</th>\n",
       "      <th>'goodbye said</th>\n",
       "      <th>...</th>\n",
       "      <th>£8.5bn</th>\n",
       "      <th>£800m</th>\n",
       "      <th>£800m 1.5bn</th>\n",
       "      <th>£80m</th>\n",
       "      <th>£857m</th>\n",
       "      <th>£8bn</th>\n",
       "      <th>£8m</th>\n",
       "      <th>£9.4m</th>\n",
       "      <th>£99</th>\n",
       "      <th>£9m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31809 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   'best  'bot  'bot nets  'cold  'cold calls  'could  'do  'eu  'goodbye  \\\n",
       "0    0.0   0.0        0.0    0.0          0.0     0.0  0.0  0.0       0.0   \n",
       "1    0.0   0.0        0.0    0.0          0.0     0.0  0.0  0.0       0.0   \n",
       "2    0.0   0.0        0.0    0.0          0.0     0.0  0.0  0.0       0.0   \n",
       "3    0.0   0.0        0.0    0.0          0.0     0.0  0.0  0.0       0.0   \n",
       "4    0.0   0.0        0.0    0.0          0.0     0.0  0.0  0.0       0.0   \n",
       "\n",
       "   'goodbye said  ...  £8.5bn  £800m  £800m 1.5bn  £80m  £857m  £8bn  £8m  \\\n",
       "0            0.0  ...     0.0    0.0          0.0   0.0    0.0   0.0  0.0   \n",
       "1            0.0  ...     0.0    0.0          0.0   0.0    0.0   0.0  0.0   \n",
       "2            0.0  ...     0.0    0.0          0.0   0.0    0.0   0.0  0.0   \n",
       "3            0.0  ...     0.0    0.0          0.0   0.0    0.0   0.0  0.0   \n",
       "4            0.0  ...     0.0    0.0          0.0   0.0    0.0   0.0  0.0   \n",
       "\n",
       "   £9.4m  £99  £9m  \n",
       "0    0.0  0.0  0.0  \n",
       "1    0.0  0.0  0.0  \n",
       "2    0.0  0.0  0.0  \n",
       "3    0.0  0.0  0.0  \n",
       "4    0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 31809 columns]"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31809"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('16-year-old', 'depressed'), ('1990', '2004'), ('1999', '2003'), ('2', 'european'), ('2004', '25'), ('address', 'issue'), ('alerted', 'experience'), ('also', 'highlighted'), ('amount', 'serious'), ('appearance', 'series')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "doc_set = [preprocess(doc) for doc in corpus]\n",
    "finder = BigramCollocationFinder.from_words(doc_set[0])\n",
    "print(finder.nbest(bigram_measures.pmi, 10))\n",
    "#finder.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('vulnerable', 'people'), 0.014705882352941176),\n",
       " (('taking', 'life'), 0.011029411764705883),\n",
       " (('alcohol', 'problem'), 0.007352941176470588),\n",
       " (('bar', 'cell'), 0.007352941176470588),\n",
       " (('custody', 'death'), 0.007352941176470588),\n",
       " (('death', 'custody'), 0.007352941176470588),\n",
       " (('death', 'rate'), 0.007352941176470588),\n",
       " (('drug', 'alcohol'), 0.007352941176470588),\n",
       " (('highly', 'vulnerable'), 0.007352941176470588),\n",
       " (('human', 'right'), 0.007352941176470588)]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_fd = nltk.FreqDist(doc_set[0])\n",
    "bigram_fd = nltk.FreqDist(nltk.bigrams(doc_set[0]))\n",
    "finder = BigramCollocationFinder(word_fd, bigram_fd)\n",
    "finder.score_ngrams(bigram_measures.raw_freq)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('drug', 'alcohol', 'problem'), 0.007352941176470588),\n",
       " (('life', 'highly', 'vulnerable'), 0.007352941176470588),\n",
       " ((\"'shocks\", 'mp', 'death'), 0.003676470588235294),\n",
       " (('16-year-old', 'depressed', 'exhibiting'), 0.003676470588235294),\n",
       " (('1990', '2004', '25'), 0.003676470588235294),\n",
       " (('1999', '2003', 'mp'), 0.003676470588235294),\n",
       " (('2', 'european', 'convention'), 0.003676470588235294),\n",
       " (('2002', 'urged', 'home'), 0.003676470588235294),\n",
       " (('2003', 'mp', 'said'), 0.003676470588235294),\n",
       " (('2004', '25', 'child'), 0.003676470588235294)]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = TrigramCollocationFinder.from_words(doc_set[0])\n",
    "finder.score_ngrams(trigram_measures.raw_freq)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(preprocess(doc) for doc in corpus)\n",
    "bow = [dictionary.doc2bow(preprocess(doc)) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.025*\"wale\" + 0.018*\"england\" + 0.018*\"zealand\" + 0.017*\"new\" + 0.014*\"game\"'), (1, '0.023*\"band\" + 0.019*\"music\" + 0.017*\"best\" + 0.016*\"award\" + 0.013*\"album\"'), (2, '0.028*\"game\" + 0.020*\"said\" + 0.015*\"china\" + 0.013*\"lending\" + 0.012*\"sony\"'), (3, '0.017*\"ireland\" + 0.012*\"j\" + 0.011*\"minute\" + 0.009*\"g\" + 0.009*\"o\\'gara\"'), (4, '0.015*\"said\" + 0.014*\"show\" + 0.011*\"people\" + 0.009*\"bbc\" + 0.009*\"u\"'), (5, '0.015*\"said\" + 0.010*\"system\" + 0.010*\"mobile\" + 0.010*\"technology\" + 0.009*\"people\"'), (6, '0.021*\"drug\" + 0.016*\"test\" + 0.014*\"sport\" + 0.014*\"also\" + 0.014*\"greek\"'), (7, '0.021*\"said\" + 0.018*\"michael\" + 0.018*\"film\" + 0.011*\"different\" + 0.010*\"life\"'), (8, '0.022*\"film\" + 0.013*\"number\" + 0.012*\"one\" + 0.009*\"year\" + 0.007*\"award\"'), (9, '0.010*\"said\" + 0.010*\"game\" + 0.009*\"player\" + 0.008*\"time\" + 0.006*\"world\"'), (10, '0.017*\"said\" + 0.011*\"program\" + 0.009*\"microsoft\" + 0.008*\"virus\" + 0.007*\"software\"'), (11, '0.015*\"said\" + 0.009*\"year\" + 0.007*\"carry\" + 0.006*\"found\" + 0.006*\"like\"'), (12, '0.016*\"said\" + 0.013*\"government\" + 0.011*\"year\" + 0.010*\"company\" + 0.010*\"mr\"'), (13, '0.018*\"said\" + 0.011*\"year\" + 0.010*\"u\" + 0.008*\"market\" + 0.008*\"dollar\"'), (14, '0.038*\"scottish\" + 0.025*\"bp\" + 0.013*\"glasgow\" + 0.011*\"public\" + 0.010*\"scotland\"'), (15, '0.019*\"said\" + 0.015*\"mr\" + 0.013*\"v\" + 0.007*\"united\" + 0.005*\"50\"'), (16, '0.027*\"mr\" + 0.022*\"said\" + 0.013*\"tory\" + 0.013*\"labour\" + 0.012*\"party\"'), (17, '0.021*\"mobile\" + 0.018*\"phone\" + 0.014*\"music\" + 0.014*\"said\" + 0.011*\"people\"'), (18, '0.033*\"said\" + 0.012*\"police\" + 0.011*\"new\" + 0.011*\"law\" + 0.010*\"mr\"'), (19, '0.014*\"site\" + 0.011*\"said\" + 0.009*\"hip-hop\" + 0.008*\"world\" + 0.007*\"say\"')]\n"
     ]
    }
   ],
   "source": [
    "ldamodel = models.ldamodel.LdaModel(bow, num_topics=20, id2word=dictionary, passes=50, minimum_probability=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, '0.022*\"film\" + 0.013*\"number\" + 0.012*\"one\" + 0.009*\"year\" + 0.007*\"award\" + 0.007*\"best\" + 0.007*\"director\" + 0.006*\"new\" + 0.006*\"chart\" + 0.006*\"single\" + 0.006*\"week\" + 0.006*\"festival\" + 0.006*\"said\" + 0.006*\"first\" + 0.005*\"also\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=1, num_words=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', \n",
    "                                                                binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('teenage_girl', 0.626004159450531), ('girl', 0.5984843969345093), ('teenager', 0.5653390884399414), ('boy', 0.5254422426223755), ('policewoman', 0.5163928866386414), ('Woman', 0.5034411549568176), ('person', 0.5024771690368652), ('teenaged_girl', 0.4996837377548218), ('female_jogger', 0.49290722608566284), ('motorist', 0.4890908896923065)]\n"
     ]
    }
   ],
   "source": [
    "analogy = wv_embeddings.most_similar(positive=['man', 'woman'], negative=['king'])\n",
    "print(analogy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lenguaje', 0.6201727986335754), ('debajo_de', 0.6004170775413513), ('largo_de', 0.5995573997497559), ('precio_windows_7', 0.5937506556510925), ('otros', 0.592975914478302), ('Aunque', 0.5886745452880859), ('de_datos', 0.5863443613052368), ('peru', 0.5859358906745911), ('diferentes', 0.5850200653076172), ('para_nosotros', 0.5848908424377441)]\n"
     ]
    }
   ],
   "source": [
    "similar_word = wv_embeddings.most_similar('colombia')\n",
    "print(similar_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phraser, Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data - remove punctuation from every text\n",
    "texts = corpus.copy()\n",
    "sentences = []\n",
    "# Go through each text in turn\n",
    "for ii in range(len(texts)):\n",
    "    sentences = [re.sub(pattern=r'[\\!\"#$%&\\*+,-./:;<=>?@^_`()|~=]', \n",
    "                        repl='', \n",
    "                        string=x\n",
    "                       ).strip().split(' ') for x in texts[ii].split('\\n') \n",
    "                      if not x.endswith('writes:')]\n",
    "    sentences = [x for x in sentences if x != ['']]\n",
    "    texts[ii] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all sentences from all texts into a single list of sentences\n",
    "all_sentences = []\n",
    "for text in texts:\n",
    "    all_sentences += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrase Detection\n",
    "# Give some common terms that can be ignored in phrase detection\n",
    "# For example, 'state_of_affairs' will be detected because 'of' is provided here: \n",
    "common_terms = [\"of\", \"with\", \"without\", \"and\", \"or\", \"the\", \"a\"]\n",
    "# Create the relevant phrases from the list of sentences:\n",
    "phrases = Phrases(all_sentences, common_terms=common_terms)\n",
    "# The Phraser object is used from now on to transform sentences\n",
    "bigram = Phraser(phrases)\n",
    "# Applying the Phraser to transform our sentences is simply\n",
    "all_sentences = list(bigram[all_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time:  50.80308508872986\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "model = Word2Vec(all_sentences, \n",
    "                 min_count=2,   # Ignore words that appear less than this\n",
    "                 size=200,      # Dimensionality of word embeddings\n",
    "                 workers=8,     # Number of processors (parallelisation)\n",
    "                 window=5,      # Context window for words during training\n",
    "                 iter=100)       # Number of epochs training over corpus\n",
    "end = time.time()\n",
    "print('Train time: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('iTunes', 0.41001155972480774),\n",
       " ('Motorola', 0.3779086172580719),\n",
       " ('Microsoft', 0.3739736080169678),\n",
       " ('Nintendo', 0.3733164370059967),\n",
       " ('iPod', 0.3729538917541504),\n",
       " (\"Apple's\", 0.36621707677841187),\n",
       " ('tool', 0.3646322786808014),\n",
       " ('Internet_Explorer', 0.3623224198818207),\n",
       " ('operating_system', 0.3539520502090454),\n",
       " ('Firefox', 0.34643125534057617)]"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('Apple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(corpus[0])\n",
    "total_documents = len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_frequency_matrix(sentences, stopWords):\n",
    "    frequency_matrix = {}\n",
    "    lem = lemmatizer\n",
    "\n",
    "    for sent in sentences:\n",
    "        freq_table = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = lem.lemmatize(word)\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "    \n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "        frequency_matrix[sent[:15]] = freq_table\n",
    "\n",
    "    return frequency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_matrix = sentence_frequency_matrix(sentences, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tf_matrix(freq_matrix):\n",
    "    tf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        tf_table = {}\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, count in f_table.items():\n",
    "            tf_table[word] = count / count_words_in_sentence\n",
    "\n",
    "        tf_matrix[sent] = tf_table\n",
    "\n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix = sentence_tf_matrix(freq_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_df_matrix(freq_matrix):\n",
    "    df_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        for word, count in f_table.items():\n",
    "            if word in df_matrix:\n",
    "                df_matrix[word] += 1\n",
    "            else:\n",
    "                df_matrix[word] = 1\n",
    "\n",
    "    return df_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matrix = sentence_df_matrix(freq_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_idf_matrix(freq_matrix, df_matrix, total_documents):\n",
    "    import math\n",
    "    idf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        idf_table = {}\n",
    "\n",
    "        for word in f_table.keys():\n",
    "            idf_table[word] = math.log10(total_documents / float(df_matrix[word]))\n",
    "\n",
    "        idf_matrix[sent] = idf_table\n",
    "\n",
    "    return idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_matrix = sentence_idf_matrix(freq_matrix, df_matrix, total_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tf_idf_matrix(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "\n",
    "    for (sent1, tf_table), (sent2, idf_table) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "\n",
    "        tf_idf_table = {}\n",
    "\n",
    "        for (word1, value1), (word2, value2) in zip(tf_table.items(), \n",
    "                                                    idf_table.items()):  # keys are same: word1 == word2\n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "\n",
    "        tf_idf_matrix[sent1] = tf_idf_table\n",
    "\n",
    "    return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix = sentence_tf_idf_matrix(tf_matrix, idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentences(tf_idf_matrix):\n",
    "    sentence_scores = {}\n",
    "\n",
    "    for sent, f_table in tf_idf_matrix.items():\n",
    "        total_score_per_sentence = 0\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, score in f_table.items():\n",
    "            total_score_per_sentence += score\n",
    "\n",
    "        sentence_scores[sent] = total_score_per_sentence / count_words_in_sentence\n",
    "\n",
    "    return sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = score_sentences(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Custody death r': 0.10179248221200314, 'The joint commi': 0.06977472553820856, 'Members urged t': 0.07481870532259614, 'There was one p': 0.0903398240544809, 'The report, whi': 0.056499799137089766, 'Many of those w': 0.08823013392966018, 'It questioned w': 0.10289222174044234, 'Increased resou': 0.089557625403835, 'Committee chair': 0.08243265509226147, '\"Yet throughout': 0.05409358357349983, '\"These highly v': 0.09649459673957755, '\"Crime levels a': 0.14957262692375967, 'The misplaced o': 0.10152967460208544, '\"Until we chang': 0.06723165486153318, 'The committee a': 0.0888845243928637, 'Between 1990 an': 0.09034930525444314, 'It picked out t': 0.058903018563800966, 'It revealed tha': 0.06826391952557925, 'Even though the': 0.07132881808537661, 'Nine days into ': 0.12751924344770646}\n"
     ]
    }
   ],
   "source": [
    "print(sentence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_average_score(sentence_scores):\n",
    "    sumValues = 0\n",
    "    for entry in sentence_scores:\n",
    "        sumValues += sentence_scores[entry]\n",
    "\n",
    "    # Average value of a sentence from original summary_text\n",
    "    average = (sumValues / len(sentence_scores))\n",
    "\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = calc_average_score(sentence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08652545692004016"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(sentences, sentence_scores, threshold):\n",
    "    sentence_count = 0\n",
    "    summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence[:15] in sentence_scores and sentence_scores[sentence[:15]] >= (threshold):\n",
    "            summary += \" \" + sentence\n",
    "            sentence_count += 1\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Custody death rate 'shocks' MPs\n",
      "\n",
      "Deaths in custody have reached \"shocking\" levels, a committee of MPs and peers has warned. The joint committee on human rights found those committing suicide were mainly the most vulnerable, with mental health, drugs or alcohol problems. Members urged the government to set up a task force to tackle deaths in prisons, police cells, detention centres and special hospitals. There was one prison suicide every four days between 1999 and 2003, MPs said. The report, which followed a year-long inquiry by the committee, found the high death rate \"amounts to a serious failure to protect the right to life of a highly vulnerable group\". Many of those who ended up taking their own lives had \"presented themselves\" to the authorities with these problems before they even offended, the report said. It questioned whether prison was the most appropriate place for them to be kept and whether earlier intervention would have meant custody could have been avoided. Increased resources and a reduction in the use of imprisonment was needed to address the issue in the longer term, the report said. Committee chairman Labour MP Jean Corston said: \"Each and every death in custody is a death too many, regardless of the circumstances. \"Yet throughout our inquiry we have seen time and time again that extremely vulnerable people are entering custody with a history of mental illness, drug and alcohol problems and potential for taking their own lives.\" \"These highly vulnerable people are being held within a structure glaringly ill-suited to meet even their basic needs. \"Crime levels are falling but we are holding more people in custody than ever before. The misplaced over-reliance on the prison system for some of the most vulnerable people in the country is at the heart of the problems that we encountered. \"Until we change our whole approach to imprisoning vulnerable people we cannot begin to meet our positive obligations under Article 2 of the European Convention on Human Rights and meet our duty of care to them.\" The committee also highlighted \"deeply worrying\" cases of children and young people taking their own lives. Between 1990 and 2004, 25 children have taken their own lives in prison and two have died in secure training centres. It picked out the case of Joseph Scholes, who hanged himself from the bars of his cell in Stoke Heath Young Offender Institution in March 2002, and urged the home secretary to hold a public inquiry. It revealed that two weeks before his court appearance for a series of robberies, the 16-year-old was depressed, exhibiting suicidal tendencies and slashed his face with a knife about 30 times. Even though the trial judge had been alerted to his experience of sexual abuse and mental illness, he was sentenced to a two-year detention and training order. Nine days into his sentence, Joseph hung himself from the bars of his cell window with a sheet.\n"
     ]
    }
   ],
   "source": [
    "original_text = generate_summary(sentences, sentence_scores, 0.0 * threshold)\n",
    "print(original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"Crime levels are falling but we are holding more people in custody than ever before. Nine days into his sentence, Joseph hung himself from the bars of his cell window with a sheet.\n"
     ]
    }
   ],
   "source": [
    "summary = generate_summary(sentences, sentence_scores, 1.2 * threshold)\n",
    "print((summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
